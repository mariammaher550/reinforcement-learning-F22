{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "VawzvYeglFNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from typing import Type\n"
      ],
      "metadata": {
        "id": "i1pDayR_lKjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [VGG](https://arxiv.org/pdf/1409.1556.pdf)\n",
        "\n",
        "Implement VGG16, for that write specific `nn.Module`, `VGGBlock` implementing block of VGG."
      ],
      "metadata": {
        "id": "od-IHzGCC2sr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITvQtQzDC1MN"
      },
      "outputs": [],
      "source": [
        "# Based on : https://github.com/msyim/VGG16\n",
        "\n",
        "def conv_layer(chann_in, chann_out, k_size, p_size):\n",
        "    layer = tnn.Sequential(\n",
        "        tnn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),\n",
        "        tnn.BatchNorm2d(chann_out),\n",
        "        tnn.ReLU()\n",
        "    )\n",
        "    return layer\n",
        "\n",
        "def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):\n",
        "\n",
        "    layers = [ conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list)) ]\n",
        "    layers += [ tnn.MaxPool2d(kernel_size = pooling_k, stride = pooling_s)]\n",
        "    return tnn.Sequential(*layers)\n",
        "\n",
        "def vgg_fc_layer(size_in, size_out):\n",
        "    layer = tnn.Sequential(\n",
        "        tnn.Linear(size_in, size_out),\n",
        "        tnn.BatchNorm1d(size_out),\n",
        "        tnn.ReLU()\n",
        "    )\n",
        "    return layer\n",
        "\n",
        "class VGG16(tnn.Module):\n",
        "    def __init__(self, n_classes=1000):\n",
        "        super(VGG16, self).__init__()\n",
        "\n",
        "        # Conv blocks (BatchNorm + ReLU activation added in each block)\n",
        "        self.layer1 = vgg_conv_block([3,64], [64,64], [3,3], [1,1], 2, 2)\n",
        "        self.layer2 = vgg_conv_block([64,128], [128,128], [3,3], [1,1], 2, 2)\n",
        "        self.layer3 = vgg_conv_block([128,256,256], [256,256,256], [3,3,3], [1,1,1], 2, 2)\n",
        "        self.layer4 = vgg_conv_block([256,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)\n",
        "        self.layer5 = vgg_conv_block([512,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)\n",
        "\n",
        "        # FC layers\n",
        "        self.layer6 = vgg_fc_layer(7*7*512, 4096)\n",
        "        self.layer7 = vgg_fc_layer(4096, 4096)\n",
        "\n",
        "        # Final layer\n",
        "        self.layer8 = tnn.Linear(4096, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        vgg16_features = self.layer5(out)\n",
        "        out = vgg16_features.view(out.size(0), -1)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "\n",
        "        return vgg16_features, out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [GoogLeNet](https://arxiv.org/pdf/1409.4842.pdf)\n",
        "\n",
        "## Inception module\n",
        "\n",
        "Write specific `nn.Module` for Inception module."
      ],
      "metadata": {
        "id": "Gi43r1dPDRp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Based on https://sahiltinky94.medium.com/know-about-googlenet-and-implementation-using-pytorch-92f827d675db\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self, curr_in_fts, f_1x1, f_3x3_r, f_3x3, f_5x5_r, f_5x5, f_pool_proj):\n",
        "        super(InceptionModule, self).__init__()\n",
        "        self.conv1 = ConvBlock(curr_in_fts, f_1x1, 1, 1, 0)\n",
        "        self.conv2 = ReduceConvBlock(curr_in_fts, f_3x3_r, f_3x3, 3, 1)\n",
        "        self.conv3 = ReduceConvBlock(curr_in_fts, f_5x5_r, f_5x5, 5, 2)\n",
        "\n",
        "        self.pool_proj = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.Conv2d(in_channels=curr_in_fts, out_channels=f_pool_proj, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        out1 = self.conv1(inputs)\n",
        "        out2 = self.conv2(inputs)\n",
        "        out3 = self.conv3(inputs)\n",
        "        out4 = self.pool_proj(inputs)\n",
        "\n",
        "        x = torch.cat([out1, out2, out3, out4], dim=1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_fts, out_fts, k, s, p):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.structure = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_fts, out_channels=out_fts, kernel_size=(k, k), stride=(s, s), padding=(p, p)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.structure(inputs)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ReduceConvBlock(nn.Module):\n",
        "    def __init__(self, in_fts, out_fts_1, out_fts_2, k, p):\n",
        "        super(ReduceConvBlock, self).__init__()\n",
        "        self.redConv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_fts, out_channels=out_fts_1, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=out_fts_1, out_channels=out_fts_2, kernel_size=(k, k), stride=(1, 1), padding=(p, p)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.redConv(inputs)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "L91nFCXhDhxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stem network\n",
        "\n",
        "Write down, why do we need a Stem network."
      ],
      "metadata": {
        "id": "Pot5itHXDisN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Write your answer here.*\n",
        "\n",
        "\n",
        "Quickly downsample an input with strided convolutions with quite large kernel size, so that further layers can effectively do their work with much less computational complexity. Based on [this](https://stackoverflow.com/questions/68258188/why-does-cnns-usually-have-a-stem) and [this](https://www.researchgate.net/figure/The-architecture-of-ResNet-50-vd-a-Stem-block-b-Stage1-Block1-c-Stage1-Block2_fig4_349646156). So basically just to downsample the input.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nhADh2IVDojD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ResNet](https://arxiv.org/pdf/1512.03385.pdf)\n",
        "\n",
        "Implement ResNet-18, for that write specific `nn.Module`, `ResNetBlock` implementing block of ResNet."
      ],
      "metadata": {
        "id": "uD7dsr2yDuGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "# Based on https://debuggercafe.com/implementing-resnet18-in-pytorch-from-scratch/\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int = 1,\n",
        "        expansion: int = 1,\n",
        "        downsample: nn.Module = None\n",
        "    ) -> None:\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # Multiplicative factor for the subsequent conv2d layer's output channels.\n",
        "        # It is 1 for ResNet18 and ResNet34.\n",
        "        self.expansion = expansion\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, \n",
        "            out_channels, \n",
        "            kernel_size=3, \n",
        "            stride=stride, \n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, \n",
        "            out_channels*self.expansion, \n",
        "            kernel_size=3, \n",
        "            padding=1,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return  out\n",
        "\n"
      ],
      "metadata": {
        "id": "FJxA2J7OEPxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        img_channels: int,\n",
        "        num_layers: int,\n",
        "        block: Type[BasicBlock],\n",
        "        num_classes: int  = 1000\n",
        "    ) -> None:\n",
        "        super(ResNet, self).__init__()\n",
        "        if num_layers == 18:\n",
        "            # The following `layers` list defines the number of `BasicBlock` \n",
        "            # to use to build the network and how many basic blocks to stack\n",
        "            # together.\n",
        "            layers = [2, 2, 2, 2]\n",
        "            self.expansion = 1\n",
        "        \n",
        "        self.in_channels = 64\n",
        "        # All ResNets (18 to 152) contain a Conv2d => BN => ReLU for the first\n",
        "        # three layers. Here, kernel size is 7.\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=img_channels,\n",
        "            out_channels=self.in_channels,\n",
        "            kernel_size=7, \n",
        "            stride=2,\n",
        "            padding=3,\n",
        "            bias=False\n",
        "        )\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512*self.expansion, num_classes)\n",
        "    def _make_layer(\n",
        "        self, \n",
        "        block: Type[BasicBlock],\n",
        "        out_channels: int,\n",
        "        blocks: int,\n",
        "        stride: int = 1\n",
        "    ) -> nn.Sequential:\n",
        "        downsample = None\n",
        "        if stride != 1:\n",
        "            \"\"\"\n",
        "            This should pass from `layer2` to `layer4` or \n",
        "            when building ResNets50 and above. Section 3.3 of the paper\n",
        "            Deep Residual Learning for Image Recognition\n",
        "            (https://arxiv.org/pdf/1512.03385v1.pdf).\n",
        "            \"\"\"\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    self.in_channels, \n",
        "                    out_channels*self.expansion,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False \n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(\n",
        "                self.in_channels, out_channels, stride, self.expansion, downsample\n",
        "            )\n",
        "        )\n",
        "        self.in_channels = out_channels * self.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(\n",
        "                self.in_channels,\n",
        "                out_channels,\n",
        "                expansion=self.expansion\n",
        "            ))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        # The spatial dimension of the final layer's feature \n",
        "        # map should be (7, 7) for all ResNets.\n",
        "        print('Dimensions of the last convolutional feature map: ', x.shape)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Xc436Zbboka6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [ResNeXt](https://arxiv.org/pdf/1611.05431.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `ResNeXtBlock` implementing block of ResNeXt."
      ],
      "metadata": {
        "id": "cAkVh_wLES6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resource: https://github.com/pytorch/vision/blob/1aef87d01eec2c0989458387fa04baebcc86ea7b/torchvision/models/resnet.py#L75\n",
        "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
        "\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class ResNextBlock(nn.Module):\n",
        "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
        "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
        "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
        "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
        "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
        "\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
        "                 base_width=64, dilation=1, norm_layer=None):\n",
        "        super(ResNextBlock, self).__init__()\n",
        "        if norm_layer is None:\n",
        "            norm_layer = nn.BatchNorm2d\n",
        "        width = int(planes * (base_width / 64.)) * groups\n",
        "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv1x1(inplanes, width)\n",
        "        self.bn1 = norm_layer(width)\n",
        "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
        "        self.bn2 = norm_layer(width)\n",
        "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
        "        self.bn3 = norm_layer(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "6FN7E2GADnTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [SENet](https://arxiv.org/pdf/1709.01507.pdf)\n",
        "\n",
        "Write specific `nn.Module`, `SEBlock` implementing block of SENet."
      ],
      "metadata": {
        "id": "ebgJY9YcEw2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Resource: https://github.com/moskomule/senet.pytorch/blob/master/senet/se_module.py\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)"
      ],
      "metadata": {
        "id": "BfddhrMkE5un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Neural Architecture Search](https://arxiv.org/pdf/1611.01578.pdf)\n",
        "\n",
        "For the neural network of your assignment 2, write down the parametrization of the network you would use for the NAS."
      ],
      "metadata": {
        "id": "px-Os_viE-aM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameters are : \\\\\n",
        "1- number of layers \\\\\n",
        "2- filter size \\\\\n",
        "3- type of activation function \\\\\n",
        "4- type of pooling \\\\\n",
        "5- stride number"
      ],
      "metadata": {
        "id": "sojINj1YFeMU"
      }
    }
  ]
}